{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from transformers import BertJapaneseTokenizer, BertModel\n",
    "import torch\n",
    "from sudachipy import dictionary\n",
    "from sudachipy import tokenizer\n",
    "import pickle\n",
    "import fasttext\n",
    "import fasttext.util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# クラスタリングの類似度を評価する関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pair_label(cluster):\n",
    "  labels = []\n",
    "  for i0, v0 in enumerate(cluster):\n",
    "    for i1, v1 in enumerate(cluster):\n",
    "      if i1<=i0: continue\n",
    "      labels.append(v0==v1)\n",
    "  return labels\n",
    "\n",
    "def cluster_similarity(correct_cluster, test_cluster):\n",
    "  correct_pairs = get_pair_label(correct_cluster)\n",
    "  test_pairs = get_pair_label(test_cluster)\n",
    "  combined_pairs = [(v0,v1) for v0, v1 in zip(correct_pairs, test_pairs)]\n",
    "\n",
    "  correct_true, correct_false = correct_pairs.count(True), correct_pairs.count(False)\n",
    "  test_true, test_false = test_pairs.count(True), test_pairs.count(False)\n",
    "  true_positive = combined_pairs.count((True, True))\n",
    "  false_positive = combined_pairs.count((False, True))\n",
    "  true_negative = combined_pairs.count((False, False))\n",
    "  false_negative = combined_pairs.count((True, False))\n",
    "\n",
    "  scores = {\n",
    "    \"ct_cf_tt_tf\": (correct_true, correct_false, test_true, test_false)\n",
    "    , \"tp_fp_tn_fn\": (true_positive, false_positive, true_negative, false_negative)\n",
    "    , \"precision\": precision_score(correct_pairs, test_pairs)\n",
    "    , \"recall\": recall_score(correct_pairs, test_pairs)\n",
    "    , \"f1\": f1_score(correct_pairs, test_pairs)\n",
    "    , \"accuracy\": accuracy_score(correct_pairs, test_pairs)\n",
    "  }\n",
    "  return scores    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ct_cf_tt_tf': (6, 9, 6, 9),\n",
       " 'tp_fp_tn_fn': (2, 4, 5, 4),\n",
       " 'precision': 0.3333333333333333,\n",
       " 'recall': 0.3333333333333333,\n",
       " 'f1': 0.3333333333333333,\n",
       " 'accuracy': 0.4666666666666667}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_similarity([0,0,0,1,1,1],[1,0,0,0,0,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = \"text/titles.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all\n",
      "sports-watch      900\n",
      "dokujo-tsushin    870\n",
      "it-life-hack      870\n",
      "movie-enter       870\n",
      "smax              870\n",
      "kaden-channel     864\n",
      "peachy            842\n",
      "topic-news        770\n",
      "livedoor-homme    511\n",
      "Name: category, dtype: int64\n",
      "\n",
      "train\n",
      "sports-watch      810\n",
      "smax              783\n",
      "it-life-hack      783\n",
      "dokujo-tsushin    783\n",
      "movie-enter       783\n",
      "kaden-channel     777\n",
      "peachy            758\n",
      "topic-news        693\n",
      "livedoor-homme    460\n",
      "Name: category, dtype: int64\n",
      "\n",
      "test\n",
      "sports-watch      90\n",
      "kaden-channel     87\n",
      "smax              87\n",
      "dokujo-tsushin    87\n",
      "movie-enter       87\n",
      "it-life-hack      87\n",
      "peachy            84\n",
      "topic-news        77\n",
      "livedoor-homme    51\n",
      "Name: category, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# テストデータの読み込み\n",
    "df = pd.read_csv(FILE_PATH)\n",
    "# 検証を素早くできるようにテストデータ数を制限\n",
    "train_df, test_df = train_test_split(df, train_size=0.9, random_state = 0, shuffle=True, stratify=df[\"category\"])\n",
    "# indexをリセット\n",
    "train_df, test_df = train_df.reset_index(drop=True), test_df.reset_index(drop=True)\n",
    "\n",
    "print(\"all\")\n",
    "print(df[\"category\"].value_counts())\n",
    "print(\"\")\n",
    "print(\"train\")\n",
    "print(train_df[\"category\"].value_counts())\n",
    "print(\"\")\n",
    "print(\"test\")\n",
    "print(test_df[\"category\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding取得のための関数の定義"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf\n",
    "def ngram_tfidf(texts, *, ngram_range = (3,3)):\n",
    "  vectorizer = TfidfVectorizer(\n",
    "                    analyzer=\"char\"\n",
    "                    , ngram_range=ngram_range\n",
    "                    , max_df=0.9\n",
    "                    , min_df = 5)\n",
    "  return vectorizer.fit_transform(texts)\n",
    "\n",
    "def word_tfidf(texts, *, ngram_range = (1,1)):\n",
    "  tokenizer_obj = dictionary.Dictionary(dict=\"full\").create()\n",
    "  mode = tokenizer.Tokenizer.SplitMode.A\n",
    "  wakachi_texts = [\" \".join([m.surface() for m in tokenizer_obj.tokenize(text, mode)]) for text in texts]\n",
    "  vectorizer = TfidfVectorizer(\n",
    "    analyzer = \"word\"\n",
    "    , ngram_range = ngram_range\n",
    "    , max_df = 0.9\n",
    "    , min_df = 5\n",
    "  )\n",
    "  return vectorizer.fit_transform(wakachi_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fasttext_vector(texts, *, model=None, model_path = \"fasttext/cc.ja.300.bin\"):\n",
    "  ft = model or fasttext.load_model(model_path)\n",
    "  tokenizer_obj = dictionary.Dictionary(dict=\"full\").create()\n",
    "  mode = tokenizer.Tokenizer.SplitMode.A\n",
    "  vectors = []\n",
    "  for text in texts:\n",
    "    tokens = tokenizer_obj.tokenize(text)\n",
    "    words = [token.surface() for token in tokens]\n",
    "    vec = ft.get_word_vector(words[0])\n",
    "    for w in words[1:]:\n",
    "      vec += ft.get_word_vector(w)\n",
    "    mean_vec = vec / len(words)\n",
    "    vectors.append(mean_vec)\n",
    "  return vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence-Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参考 https://qiita.com/sonoisa/items/1df94d0a98cd4f209051\n",
    "class SentenceBertJapanese:\n",
    "    def __init__(self, model_name_or_path, device=None):\n",
    "        self.tokenizer = BertJapaneseTokenizer.from_pretrained(model_name_or_path)\n",
    "        self.model = BertModel.from_pretrained(model_name_or_path)\n",
    "        self.model.eval()\n",
    "\n",
    "        if device is None:\n",
    "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.device = torch.device(device)\n",
    "        self.model.to(device)\n",
    "\n",
    "    def _mean_pooling(self, model_output, attention_mask):\n",
    "        token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "    def encode(self, sentences, batch_size=8):\n",
    "        all_embeddings = []\n",
    "        iterator = range(0, len(sentences), batch_size)\n",
    "        for batch_idx in iterator:\n",
    "            batch = sentences[batch_idx:batch_idx + batch_size]\n",
    "\n",
    "            encoded_input = self.tokenizer.batch_encode_plus(batch, padding=\"longest\", \n",
    "                                           truncation=True, return_tensors=\"pt\").to(self.device)\n",
    "            model_output = self.model(**encoded_input)\n",
    "            sentence_embeddings = self._mean_pooling(model_output, encoded_input[\"attention_mask\"]).to('cpu')\n",
    "\n",
    "            all_embeddings.extend(sentence_embeddings)\n",
    "\n",
    "        # return torch.stack(all_embeddings).numpy()\n",
    "        return torch.stack(all_embeddings)\n",
    "\n",
    "def sentencebert(texts, *, model=None):\n",
    "    MODEL_NAME = \"sonoisa/sentence-bert-base-ja-mean-tokens-v2\"  # <- v2です。\n",
    "    model = model or SentenceBertJapanese(MODEL_NAME)\n",
    "    sentence_embeddings = model.encode(texts, batch_size=8)\n",
    "    return sentence_embeddings.detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering用の関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-meansでクラスタ分析。とりあえず3つのグループに分けてみる\n",
    "def kmeans_clustering(vectors, *, n_clusters=9):\n",
    "  km_model = KMeans(n_clusters=n_clusters, random_state = 0)\n",
    "  km_model.fit(vectors)\n",
    "  return km_model.labels_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ngram, tfidf, kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ct_cf_tt_tf': (30397, 240819, 172013, 99203),\n",
       " 'tp_fp_tn_fn': (21785, 150228, 90591, 8612),\n",
       " 'precision': 0.12664740455663234,\n",
       " 'recall': 0.7166825673586209,\n",
       " 'f1': 0.21525616323304184,\n",
       " 'accuracy': 0.4143413367942894}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ngram tfidf, kmeans\n",
    "X = ngram_tfidf(test_df[\"title\"])\n",
    "test_labels = kmeans_clustering(X)\n",
    "cluster_similarity(test_df[\"category\"], test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word, tfidf, kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ct_cf_tt_tf': (30397, 240819, 85461, 185755),\n",
       " 'tp_fp_tn_fn': (12802, 72659, 168160, 17595),\n",
       " 'precision': 0.14979932366810592,\n",
       " 'recall': 0.4211599828930487,\n",
       " 'f1': 0.22099466588409952,\n",
       " 'accuracy': 0.667224647513421}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word tfidf, kmeans\n",
    "X = word_tfidf(test_df[\"title\"])\n",
    "test_labels = kmeans_clustering(X)\n",
    "cluster_similarity(test_df[\"category\"], test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fasttext, kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ct_cf_tt_tf': (30397, 240819, 33096, 238120),\n",
       " 'tp_fp_tn_fn': (6382, 26714, 214105, 24015),\n",
       " 'precision': 0.19283297075175249,\n",
       " 'recall': 0.20995492976280555,\n",
       " 'f1': 0.2010300348069866,\n",
       " 'accuracy': 0.812957200165182}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ft = fasttext.load_model('fasttext/cc.ja.300.bin')\n",
    "vectors = fasttext_vector(test_df[\"title\"], model = ft)\n",
    "test_labels = kmeans_clustering(vectors)\n",
    "cluster_similarity(test_df[\"category\"], test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sentencebert, kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ct_cf_tt_tf': (30397, 240819, 32532, 238684),\n",
       " 'tp_fp_tn_fn': (8420, 24112, 216707, 21977),\n",
       " 'precision': 0.25882208287224884,\n",
       " 'recall': 0.277001019837484,\n",
       " 'f1': 0.2676031718285687,\n",
       " 'accuracy': 0.8300653353784437}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentence bert, kmeans\n",
    "embedding_binary_path = \"embedding/sentencebert_embedding.pickle\"\n",
    "if Path(embedding_binary_path).exists():\n",
    "  with open(embedding_binary_path, \"rb\") as f:\n",
    "    sentence_embeddings = pickle.load(f)\n",
    "else:\n",
    "  sentence_embeddings = sentencebert(test_df[\"title\"])\n",
    "  with open(\"embedding/sentencebert_embedding.pickle\", \"wb\") as f:\n",
    "    pickle.dump(sentence_embeddings.detach().numpy(), f)\n",
    "\n",
    "test_labels = kmeans_clustering(sentence_embeddings)\n",
    "cluster_similarity(test_df[\"category\"], test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考：教師あり（トリグラム、ナイーブベイズ）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[SudachiPy](https://github.com/WorksApplications/SudachiPy/blob/develop/docs/tutorial.md)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ee66785c2d12082be357f60b53b6454e1233dc7704302fcc0105c907ee27f10e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
