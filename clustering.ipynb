{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from transformers import BertJapaneseTokenizer, BertModel\n",
    "import torch\n",
    "from sudachipy import dictionary\n",
    "from sudachipy import tokenizer\n",
    "import pickle\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "import bcubed\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# クラスタリングの類似度を評価する関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting Pair based f-measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cluster_similarity_pair(correct_cluster, test_cluster):\n",
    "  def get_pair_label(cluster):\n",
    "    labels = []\n",
    "    for i0, v0 in enumerate(cluster):\n",
    "      for i1, v1 in enumerate(cluster):\n",
    "        if i1<=i0: continue\n",
    "        labels.append(v0==v1)\n",
    "    return labels\n",
    "\n",
    "  correct_pairs = get_pair_label(correct_cluster)\n",
    "  test_pairs = get_pair_label(test_cluster)\n",
    "  combined_pairs = [(v0,v1) for v0, v1 in zip(correct_pairs, test_pairs)]\n",
    "\n",
    "  correct_true, correct_false = correct_pairs.count(True), correct_pairs.count(False)\n",
    "  test_true, test_false = test_pairs.count(True), test_pairs.count(False)\n",
    "  true_positive = combined_pairs.count((True, True))\n",
    "  false_positive = combined_pairs.count((False, True))\n",
    "  true_negative = combined_pairs.count((False, False))\n",
    "  false_negative = combined_pairs.count((True, False))\n",
    "\n",
    "  scores = {\n",
    "    \"ct_cf_tt_tf\": (correct_true, correct_false, test_true, test_false)\n",
    "    , \"tp_fp_tn_fn\": (true_positive, false_positive, true_negative, false_negative)\n",
    "    , \"precision\": metrics.precision_score(correct_pairs, test_pairs)\n",
    "    , \"recall\": metrics.recall_score(correct_pairs, test_pairs)\n",
    "    , \"f1\": metrics.f1_score(correct_pairs, test_pairs)\n",
    "    , \"accuracy\": metrics.accuracy_score(correct_pairs, test_pairs)\n",
    "  }\n",
    "  return scores    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ct_cf_tt_tf': (6, 9, 6, 9),\n",
       " 'tp_fp_tn_fn': (2, 4, 5, 4),\n",
       " 'precision': 0.3333333333333333,\n",
       " 'recall': 0.3333333333333333,\n",
       " 'f1': 0.3333333333333333,\n",
       " 'accuracy': 0.4666666666666667}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_similarity_pair([0,0,0,1,1,1],[1,0,0,0,0,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purity-Inverse Purity F-measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参考 https://stackoverflow.com/questions/34047540/python-clustering-purity-metric\n",
    "def cluster_similarity_purity(correct_cluster, test_cluster):\n",
    "  # compute contingency matrix (also called confusion matrix)\n",
    "  contingency_matrix = metrics.cluster.contingency_matrix(correct_cluster, test_cluster)\n",
    "  # purity\n",
    "  purity = np.sum(np.amax(contingency_matrix, axis=0)) / np.sum(contingency_matrix)\n",
    "  inverse_purity = np.sum(np.amax(contingency_matrix, axis=1)) / np.sum(contingency_matrix)\n",
    "\n",
    "  f1 = 2*purity*inverse_purity/(purity+inverse_purity)\n",
    "\n",
    "  score = {\n",
    "    \"precision\": purity\n",
    "    , \"recall\": inverse_purity\n",
    "    , \"f1\": f1\n",
    "  }\n",
    "  return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 0.6666666666666666,\n",
       " 'recall': 0.6666666666666666,\n",
       " 'f1': 0.6666666666666666}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_similarity_purity([0,0,0,1,1,1],[1,0,0,0,0,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BCubed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_similarity_bcubed(correct_cluster, test_cluster):\n",
    "  def get_bcubed_input(cluster):\n",
    "    labels = {i: {v} for i,v in enumerate(cluster)}\n",
    "    return labels\n",
    "\n",
    "  correct_bcubed_input = get_bcubed_input(correct_cluster)\n",
    "  test_bcubed_input = get_bcubed_input(test_cluster)\n",
    "\n",
    "  precision = bcubed.precision(correct_bcubed_input, test_bcubed_input)\n",
    "  recall = bcubed.recall(correct_bcubed_input, test_bcubed_input)\n",
    "  f1 = bcubed.fscore(precision, recall)\n",
    "  \n",
    "  scores = {\n",
    "    \"precision\": precision\n",
    "    , \"recall\": recall\n",
    "    , \"f1\": f1\n",
    "  }\n",
    "  return scores    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 0.5555555555555555,\n",
       " 'recall': 0.6666666666666666,\n",
       " 'f1': 0.606060606060606}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_similarity_bcubed([0,0,0,1,1,1],[1,0,0,0,0,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = \"text/titles.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all\n",
      "sports-watch      900\n",
      "dokujo-tsushin    870\n",
      "it-life-hack      870\n",
      "movie-enter       870\n",
      "smax              870\n",
      "kaden-channel     864\n",
      "peachy            842\n",
      "topic-news        770\n",
      "livedoor-homme    511\n",
      "Name: category, dtype: int64\n",
      "\n",
      "train\n",
      "sports-watch      810\n",
      "smax              783\n",
      "it-life-hack      783\n",
      "dokujo-tsushin    783\n",
      "movie-enter       783\n",
      "kaden-channel     777\n",
      "peachy            758\n",
      "topic-news        693\n",
      "livedoor-homme    460\n",
      "Name: category, dtype: int64\n",
      "\n",
      "test\n",
      "sports-watch      90\n",
      "kaden-channel     87\n",
      "smax              87\n",
      "dokujo-tsushin    87\n",
      "movie-enter       87\n",
      "it-life-hack      87\n",
      "peachy            84\n",
      "topic-news        77\n",
      "livedoor-homme    51\n",
      "Name: category, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# テストデータの読み込み\n",
    "df = pd.read_csv(FILE_PATH)\n",
    "# 検証を素早くできるようにテストデータ数を制限\n",
    "train_df, test_df = train_test_split(df, train_size=0.9, random_state = 0, shuffle=True, stratify=df[\"category\"])\n",
    "# indexをリセット\n",
    "train_df, test_df = train_df.reset_index(drop=True), test_df.reset_index(drop=True)\n",
    "\n",
    "print(\"all\")\n",
    "print(df[\"category\"].value_counts())\n",
    "print(\"\")\n",
    "print(\"train\")\n",
    "print(train_df[\"category\"].value_counts())\n",
    "print(\"\")\n",
    "print(\"test\")\n",
    "print(test_df[\"category\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding取得のための関数の定義"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf\n",
    "def ngram_tfidf(texts, *, ngram_range = (3,3)):\n",
    "  vectorizer = TfidfVectorizer(\n",
    "                    analyzer=\"char\"\n",
    "                    , ngram_range=ngram_range\n",
    "                    , max_df=0.9\n",
    "                    , min_df = 5)\n",
    "  return vectorizer.fit_transform(texts)\n",
    "\n",
    "def word_tfidf(texts, *, ngram_range = (1,1)):\n",
    "  tokenizer_obj = dictionary.Dictionary(dict=\"full\").create()\n",
    "  mode = tokenizer.Tokenizer.SplitMode.A\n",
    "  wakachi_texts = [\" \".join([m.surface() for m in tokenizer_obj.tokenize(text, mode)]) for text in texts]\n",
    "  vectorizer = TfidfVectorizer(\n",
    "    analyzer = \"word\"\n",
    "    , ngram_range = ngram_range\n",
    "    , max_df = 0.9\n",
    "    , min_df = 5\n",
    "  )\n",
    "  return vectorizer.fit_transform(wakachi_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fasttext_vector(texts, *, model=None, model_path = \"fasttext/cc.ja.300.bin\"):\n",
    "  ft = model or fasttext.load_model(model_path)\n",
    "  tokenizer_obj = dictionary.Dictionary(dict=\"full\").create()\n",
    "  mode = tokenizer.Tokenizer.SplitMode.A\n",
    "  vectors = []\n",
    "  for text in texts:\n",
    "    tokens = tokenizer_obj.tokenize(text)\n",
    "    words = [token.surface() for token in tokens]\n",
    "    vec = ft.get_word_vector(words[0])\n",
    "    for w in words[1:]:\n",
    "      vec += ft.get_word_vector(w)\n",
    "    mean_vec = vec / len(words)\n",
    "    vectors.append(mean_vec)\n",
    "  return vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence-Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参考 https://qiita.com/sonoisa/items/1df94d0a98cd4f209051\n",
    "class SentenceBertJapanese:\n",
    "    def __init__(self, model_name_or_path, device=None):\n",
    "        self.tokenizer = BertJapaneseTokenizer.from_pretrained(model_name_or_path)\n",
    "        self.model = BertModel.from_pretrained(model_name_or_path)\n",
    "        self.model.eval()\n",
    "\n",
    "        if device is None:\n",
    "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.device = torch.device(device)\n",
    "        self.model.to(device)\n",
    "\n",
    "    def _mean_pooling(self, model_output, attention_mask):\n",
    "        token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "    def encode(self, sentences, batch_size=8):\n",
    "        all_embeddings = []\n",
    "        iterator = range(0, len(sentences), batch_size)\n",
    "        for batch_idx in iterator:\n",
    "            batch = sentences[batch_idx:batch_idx + batch_size]\n",
    "\n",
    "            encoded_input = self.tokenizer.batch_encode_plus(batch, padding=\"longest\", \n",
    "                                           truncation=True, return_tensors=\"pt\").to(self.device)\n",
    "            model_output = self.model(**encoded_input)\n",
    "            sentence_embeddings = self._mean_pooling(model_output, encoded_input[\"attention_mask\"]).to('cpu')\n",
    "\n",
    "            all_embeddings.extend(sentence_embeddings)\n",
    "\n",
    "        # return torch.stack(all_embeddings).numpy()\n",
    "        return torch.stack(all_embeddings)\n",
    "\n",
    "def sentencebert(texts, *, model=None):\n",
    "    MODEL_NAME = \"sonoisa/sentence-bert-base-ja-mean-tokens-v2\"  # <- v2です。\n",
    "    model = model or SentenceBertJapanese(MODEL_NAME)\n",
    "    sentence_embeddings = model.encode(texts, batch_size=8)\n",
    "    return sentence_embeddings.detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([ 2.4727e-01, -3.1896e-01,  4.9332e-01, -8.4754e-02, -3.4422e-01,\n",
       "          1.0928e-01, -4.3239e-01, -2.9482e-01,  1.8846e-01, -3.3149e-01,\n",
       "         -5.4791e-01,  3.1646e-01,  4.0444e-01,  2.1014e-01,  3.1919e-01,\n",
       "         -5.7506e-01, -1.6162e-01,  2.6671e-01,  8.4874e-02,  2.6263e-01,\n",
       "         -2.7657e-01,  3.1835e-01, -6.9399e-01,  1.1396e-01, -3.9677e-01,\n",
       "         -1.6722e-01, -2.0872e-01, -2.1283e-01,  6.0268e-02, -5.7427e-03,\n",
       "          4.8959e-01, -5.0845e-01, -7.8328e-01, -6.2067e-01,  1.6136e-01,\n",
       "         -2.1672e-01,  9.6271e-03,  7.8780e-01,  1.2131e-01, -1.6523e-01,\n",
       "          8.0329e-01,  3.4971e-01, -8.1735e-01,  5.3069e-01,  1.7021e-01,\n",
       "          1.9146e-02,  3.2057e-01,  8.9604e-02,  3.7367e-01,  3.1102e-01,\n",
       "          8.8984e-01, -1.7766e-02,  2.0617e-01, -2.2226e-01,  5.1447e-01,\n",
       "          6.5785e-01, -2.2002e-01, -1.1969e-02,  5.4462e-01, -2.4289e-01,\n",
       "          8.6923e-01, -4.8421e-01, -7.2020e-02, -4.9007e-02, -2.9147e-02,\n",
       "          5.8133e-02,  6.4712e-02, -6.9244e-01, -1.4874e-01,  4.0551e-02,\n",
       "         -9.0926e-01, -1.3835e-01,  3.2343e-01,  2.3012e-01,  4.2330e-01,\n",
       "         -5.1693e-01,  3.9100e-01, -7.6036e-01,  3.0485e-02,  2.2703e-01,\n",
       "         -2.4350e-01, -9.1553e-02, -1.2496e-02, -3.8950e-01,  7.7894e-01,\n",
       "         -1.5574e-01, -8.0359e-03,  3.9446e-01, -1.9232e-01,  7.7527e-01,\n",
       "          1.2545e-02,  1.9045e-02, -3.8876e-01,  9.4335e-01, -2.4077e-02,\n",
       "         -1.2914e-01, -7.3755e-01,  7.8312e-01,  1.0762e-01, -2.7430e-01,\n",
       "          9.1311e-01,  3.9086e-01, -2.0528e-01, -5.8349e-01, -5.5975e-02,\n",
       "          3.1429e-01,  5.2305e-01,  6.7382e-01, -5.1291e-02,  1.1059e-01,\n",
       "          1.0592e-01, -4.8973e-02, -4.6002e-02,  4.5738e-02,  3.9105e-01,\n",
       "          4.7345e-01,  1.1134e+00,  4.8845e-02,  7.5611e-01,  2.4890e-01,\n",
       "         -9.2905e-02, -5.9353e-01,  8.1990e-02,  1.5494e-02,  3.4736e-01,\n",
       "          5.2146e-01,  9.7642e-01, -1.5926e-01, -6.7719e-01, -5.5041e-01,\n",
       "          2.9019e-01,  1.8310e-01,  6.2105e-01,  3.6953e-01,  6.6638e-01,\n",
       "          1.5463e-01,  2.7000e-01, -1.2585e-01, -2.2452e-01,  4.8469e-01,\n",
       "          1.8976e-01,  2.3272e-01,  1.9336e-01, -4.5299e-02, -1.5771e-01,\n",
       "          1.5231e-02, -4.9897e-01, -2.0858e-01, -2.6733e-01,  5.1714e-01,\n",
       "         -2.7133e-01, -5.6866e-01,  9.7536e-02,  1.7742e-01,  3.3324e-01,\n",
       "         -3.3844e-01, -4.4922e-01,  6.4952e-01,  5.4278e-01, -2.2436e-01,\n",
       "          6.7146e-01,  5.1947e-01,  6.0684e-02,  1.1237e-01,  8.2228e-03,\n",
       "          4.2707e-01,  1.0952e-01,  3.1071e-02, -3.4451e-01,  9.5989e-01,\n",
       "         -7.0714e-02, -8.6222e-02, -1.8075e-01, -1.6438e-01,  3.8952e-01,\n",
       "          1.0474e-01,  6.7817e-01, -8.0673e-02,  6.2616e-01,  9.4854e-02,\n",
       "         -5.4613e-01,  7.1381e-01,  2.1098e-01, -2.3402e-01, -8.9266e-01,\n",
       "         -2.0343e-01, -4.2979e-01,  1.7245e-01, -4.7638e-01, -1.3248e-01,\n",
       "         -5.6114e-01,  7.3782e-01,  1.5438e-01, -1.9516e-01,  3.4478e-01,\n",
       "         -2.4243e-01, -1.6577e-01,  4.8054e-01, -1.4287e-01, -5.1680e-02,\n",
       "          1.5448e-01, -3.8109e-01,  1.0229e+00, -1.0564e-01,  3.5493e-01,\n",
       "          1.5152e-01,  4.6670e-01, -6.6155e-01, -1.5646e-01, -4.2834e-03,\n",
       "          7.9184e-01,  5.5700e-01, -1.9811e-01,  1.3775e+00,  6.7573e-01,\n",
       "         -9.1800e-02,  1.7168e-01, -4.1670e-03,  2.7034e-01, -1.0412e+00,\n",
       "         -4.2932e-01, -5.0539e-01,  2.5666e-01, -9.5119e-02,  7.8888e-01,\n",
       "          1.8665e-02,  5.0313e-02,  3.3036e-01, -1.0005e-01,  1.3398e-01,\n",
       "         -8.9748e-01,  1.4220e-01, -6.1049e-01, -1.0783e+00,  1.4756e-01,\n",
       "          2.8708e-02,  1.8280e-01, -4.7463e-01,  8.0037e-01, -1.0317e+00,\n",
       "         -3.9960e-01,  4.0875e-01, -1.6446e-01,  1.5913e-01,  2.6615e-01,\n",
       "          4.6444e-01, -1.6876e-01,  3.0252e-01, -3.5384e-01, -5.8049e-01,\n",
       "          4.4178e-02, -1.6314e-01,  2.3611e-01, -2.6941e-01, -6.1499e-01,\n",
       "          1.9599e-01,  1.1078e-01,  7.5810e-01,  3.1020e-01,  2.3737e-02,\n",
       "         -4.6715e-01,  2.7507e-01,  2.0852e-01,  4.1291e-01, -1.1158e+00,\n",
       "         -9.0307e-02, -5.9523e-01,  1.3728e-01, -5.5147e-01,  1.6994e-01,\n",
       "          3.2985e-01, -3.5352e-01, -1.0467e-01, -1.2120e-01,  4.7904e-02,\n",
       "         -1.0049e-01,  7.6968e-02, -2.4363e-01, -1.5728e-01,  5.0506e-01,\n",
       "         -4.5910e-01,  2.5379e-01,  5.9031e-01, -9.3106e-02, -1.5248e-01,\n",
       "          3.4709e-01, -2.3040e-01,  5.7209e-01,  1.6603e-01, -2.0587e-01,\n",
       "         -2.4074e-01, -4.1124e-01, -2.3233e-01, -6.2478e-01,  3.8563e-01,\n",
       "         -5.2753e-01, -4.0229e-01, -8.4663e-03,  6.2640e-01, -4.2239e-01,\n",
       "         -7.5844e-02, -3.1355e-01, -4.7050e-01,  5.3607e-01, -1.1837e-01,\n",
       "         -4.1585e-01,  1.0400e-01, -8.8588e-01,  1.1683e-02, -5.3122e-01,\n",
       "         -3.6964e-01,  2.4637e-01,  5.6771e-01,  6.2257e-01,  7.4509e-02,\n",
       "          5.6461e-02,  3.2244e-01,  2.5397e-01, -6.2409e-01,  3.6592e-02,\n",
       "          5.9088e-01, -2.4641e-01,  1.3121e-01, -2.6976e-01,  1.1261e-01,\n",
       "          4.3267e-01, -8.4716e-02, -2.5767e-01, -7.7750e-01, -1.4720e-01,\n",
       "          7.1184e-02, -1.1749e-03,  4.9344e-01,  1.2789e-01,  2.4675e-01,\n",
       "         -1.4180e-01, -1.2502e-01,  7.8153e-01, -2.1939e-01,  7.5270e-01,\n",
       "         -2.4352e-01,  1.1806e-01,  2.8743e-01, -8.2953e-01, -3.1104e-01,\n",
       "         -2.2790e-02,  3.6491e-01, -2.4777e-01, -2.2648e-02,  1.7021e-01,\n",
       "         -4.8428e-02,  3.4201e-01,  9.5898e-02, -1.9242e-01, -4.6518e-01,\n",
       "         -9.1270e-02, -9.9675e-02,  1.3509e-01,  3.3358e-02,  2.3569e-01,\n",
       "          5.1028e-01, -4.1256e-01,  3.7224e-01, -8.9655e-01,  8.7128e-02,\n",
       "          8.1196e-01, -1.9838e-01, -3.7927e-02,  3.5187e-02, -6.0024e-01,\n",
       "          6.9540e-02, -6.1957e-01,  1.8892e-01, -4.0303e-01,  4.5522e-01,\n",
       "          5.2791e-01,  2.8650e-01,  1.8514e-01,  2.4106e-02, -7.5192e-01,\n",
       "          2.5926e-01, -1.6591e-02, -4.3513e-01,  1.9638e-01,  6.5383e-01,\n",
       "         -6.8484e-01, -9.0636e-01, -5.1915e-01,  8.9310e-02, -2.6139e-01,\n",
       "          3.3305e-01, -3.1129e-01,  5.6031e-01, -4.1700e-02, -6.9621e-01,\n",
       "          2.2563e-01,  1.1489e-01, -4.5976e-01,  6.6560e-01, -4.2844e-01,\n",
       "          1.6314e-01,  9.1337e-01, -2.4876e-02, -1.0266e-01, -1.6839e-01,\n",
       "          5.9726e-01, -9.6897e-03,  2.5630e-02, -5.4922e-01, -2.8155e-01,\n",
       "         -1.0838e-01,  3.2163e-01,  1.1861e-01, -1.1333e-01, -1.1239e+00,\n",
       "         -2.9343e-01, -3.8468e-01,  4.1092e-01,  1.7618e-01,  7.2527e-02,\n",
       "          2.8871e-01,  3.1159e-01,  1.0078e-01,  4.1102e-01, -2.0025e-01,\n",
       "         -8.9711e-02, -9.0061e-02, -6.7698e-01, -2.8927e-02, -1.0300e+00,\n",
       "         -6.1101e-01,  1.0707e+00,  5.5024e-01,  4.4929e-01, -2.8516e-01,\n",
       "          6.9951e-01,  4.4716e-01, -4.1165e-01, -1.6039e-01,  1.9420e-01,\n",
       "          7.7159e-01,  2.6066e-01, -4.0461e-01,  3.9881e-01,  8.4555e-01,\n",
       "         -8.2194e-01, -1.4411e-01,  2.3783e-01, -2.7390e-01,  1.4080e-01,\n",
       "          2.2985e-01,  7.2363e-01,  4.4016e-01, -2.3808e-02,  4.2827e-01,\n",
       "         -4.6878e-01,  1.5954e-01, -8.1558e-01,  1.4777e-01, -5.4463e-01,\n",
       "         -6.6188e-01,  1.9872e-01, -2.2918e-01,  1.2018e-01, -5.7900e-01,\n",
       "          3.5209e-02, -1.0409e+00,  3.6112e-01, -3.5237e-02, -2.9411e-02,\n",
       "         -5.5930e-02, -1.5058e-01,  4.7132e-01,  5.7357e-01, -6.5367e-01,\n",
       "          4.3510e-01,  5.2997e-02,  5.0970e-01, -4.0663e-02, -3.6144e-01,\n",
       "         -1.5805e-01, -5.8926e-01,  2.1073e-01, -3.7128e-01, -1.4450e-01,\n",
       "          1.6219e-01,  2.5114e-02, -9.6245e-01,  6.6276e-02, -9.8735e-02,\n",
       "          2.3565e-01, -4.2832e-02, -7.5281e-01, -5.8257e-01,  3.4264e-01,\n",
       "         -7.9713e-01, -2.5422e-02,  8.1736e-01,  3.7303e-01, -5.4175e-01,\n",
       "         -3.8567e-02, -1.9423e-01,  1.0872e-01, -4.9682e-01,  1.4306e-01,\n",
       "         -2.8711e-01, -4.2691e-01, -4.4504e-01,  2.6099e-01,  7.3353e-02,\n",
       "          5.5459e-01, -4.9234e-01, -7.1887e-02, -9.2941e-02,  3.4552e-02,\n",
       "         -1.6173e-01,  6.2338e-01,  1.3541e-01,  4.1356e-01, -5.6154e-02,\n",
       "          4.0660e-01,  1.5610e-01,  9.2888e-01,  5.9480e-01, -2.1058e-01,\n",
       "          1.0978e-01, -2.7986e-01, -3.4732e-01, -2.1050e-01, -1.2229e-01,\n",
       "         -1.7164e-01,  8.1949e-03, -1.3145e-02, -4.9120e-01, -2.9025e-01,\n",
       "         -2.3473e-02, -2.1344e-01, -1.0581e-01, -3.3192e-02, -2.1687e-01,\n",
       "          1.2662e-01,  4.6221e-01, -3.0382e-01, -5.1994e-01,  5.9496e-01,\n",
       "         -5.9203e-02, -2.6505e-01, -8.1369e-02,  1.1230e-02, -2.8268e-01,\n",
       "          6.3239e-01,  1.5559e-01, -7.1796e-02,  2.6106e-01,  2.7890e-01,\n",
       "         -5.0510e-01, -1.1522e-01, -3.9175e-01,  3.1910e-02,  5.2677e-01,\n",
       "         -4.3546e-02, -1.1614e-01,  2.9343e-01,  1.5415e-03, -4.7277e-01,\n",
       "          1.3683e-01,  9.0301e-02,  1.6656e-01, -3.8176e-01, -8.1213e-02,\n",
       "         -6.8489e-01, -3.4264e-01,  3.6189e-01, -6.3438e-01, -3.1357e-01,\n",
       "          3.9951e-01, -6.9605e-02, -1.4312e-01,  4.3368e-01,  6.8088e-01,\n",
       "          2.5507e-01, -5.3589e-01, -3.8470e-01, -4.2705e-01, -5.5666e-02,\n",
       "         -5.4751e-01,  1.9435e-01, -5.9134e-01,  1.7577e-01,  1.3719e-01,\n",
       "          9.0075e-02,  4.4499e-01,  2.9083e-01, -2.4076e-01,  4.0762e-01,\n",
       "          8.7407e-01, -2.2395e-01, -4.4031e-01,  1.3026e-01,  8.5839e-01,\n",
       "          4.1244e-01,  3.6598e-01, -4.1351e-01, -7.0044e-01,  7.5469e-01,\n",
       "          4.1706e-01, -1.8948e-01, -2.6014e-01,  1.1714e-01,  1.3025e-01,\n",
       "          3.4332e-02,  3.4369e-01,  5.8446e-01, -5.9309e-01,  7.5274e-01,\n",
       "         -1.1014e+00,  2.3074e-01,  7.4303e+00, -4.3688e-01,  6.4196e-02,\n",
       "         -2.0504e-01,  2.5340e-01,  7.7813e-02, -2.6499e+00,  6.3300e-01,\n",
       "         -6.1111e-01, -5.9024e-02, -2.1333e-01, -1.1413e-01, -1.2671e-01,\n",
       "          7.7772e-02, -2.1638e-01,  9.1693e-02,  2.7273e-02, -3.6425e-01,\n",
       "         -6.2383e-01,  4.8866e-01, -7.6150e-01,  3.3732e-01, -6.0769e-01,\n",
       "         -5.9679e-01,  1.8397e-01, -2.0467e-01,  2.4473e-01,  5.9063e-01,\n",
       "         -6.3819e-02, -7.8798e-01,  7.1105e-01, -2.1745e-01, -6.1137e-01,\n",
       "         -1.1347e-01,  1.0211e-01,  3.7382e-01, -2.9798e-01, -2.3862e-01,\n",
       "         -1.7237e-01,  1.9241e-01, -7.4364e-01, -9.2977e-02, -2.9862e-02,\n",
       "          2.7966e-01,  6.9399e-02, -5.7228e-01, -5.7060e-01, -2.6267e-01,\n",
       "          4.6808e-02,  3.4180e-01,  5.2892e-02, -9.2521e-01,  4.3331e-01,\n",
       "          4.0986e-01,  8.7029e-01, -6.0441e-05, -2.6956e-02, -2.6572e-01,\n",
       "          7.1216e-01, -4.9550e-01,  3.0194e-01, -4.2905e-01, -4.8039e-01,\n",
       "         -7.8136e-01, -1.3610e-01, -2.8277e-01,  5.6363e-01,  3.3272e-01,\n",
       "          2.6204e-01,  3.0941e-01,  1.5977e-01, -8.5470e-01,  3.1257e-01,\n",
       "         -7.3792e-01, -4.8851e-01, -1.0198e-01, -8.0111e-01,  3.2792e-01,\n",
       "         -5.8554e-01, -3.5260e-01,  3.5451e-01,  6.7921e-01,  2.9726e-01,\n",
       "         -3.6483e-01, -7.4339e-01,  4.6323e-02,  1.2702e-02,  3.8791e-01,\n",
       "          1.0304e+00,  5.9400e-01,  3.8405e-01,  1.8735e-01,  7.8274e-01,\n",
       "          3.0808e-01,  9.4557e-02,  3.0135e-01,  1.1746e-01, -3.9484e-01,\n",
       "         -8.4549e-02,  2.3708e-01,  3.3283e-01, -4.8951e-01,  4.7038e-02,\n",
       "         -8.6625e-02,  4.6029e-01, -1.5842e-01,  1.7834e-01, -4.1235e-01,\n",
       "         -5.7031e-01,  8.2668e-02, -9.3582e-02, -4.4848e-01, -2.4322e-01,\n",
       "          1.9650e-02, -2.4296e-01,  9.1305e-01,  1.0567e-01,  2.2296e-01,\n",
       "         -1.8936e-01,  6.2158e-01, -2.2369e-01,  1.7261e-01,  6.8938e-01,\n",
       "         -5.8412e-02, -2.9182e-01, -9.0748e-02,  1.8299e-01,  2.6977e-03,\n",
       "          2.4366e-01, -3.7565e-01,  4.9491e-01,  1.0868e-01,  3.2792e-01,\n",
       "         -2.7766e-01, -4.0152e-01, -5.2801e-02,  2.2512e-01, -2.8016e-01,\n",
       "         -9.5948e-01, -9.1715e-01,  4.9712e-01,  2.9374e-01, -3.1761e-01,\n",
       "          7.9978e-02, -2.2016e-02, -5.8037e-01, -1.5436e+00,  5.9170e-01,\n",
       "          1.0085e-01,  5.6303e-02,  2.5148e-01])]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 参考：近江崇宏; 金田健太郎; 森長誠; 江間見亜利. BERTによる自然言語処理入門 ―Transformersを使った実践プログラミング― (p.169). Kindle 版. \n",
    "#トークナイザとモデルのロード \n",
    "# 参考 https://qiita.com/sonoisa/items/1df94d0a98cd4f209051\n",
    "class BertJapanese:\n",
    "    def __init__(self, model_name_or_path, *, device=None, model = None):\n",
    "        self.tokenizer = BertJapaneseTokenizer.from_pretrained(model_name_or_path)\n",
    "        self.model = model or BertModel.from_pretrained(model_name_or_path)\n",
    "        self.model.eval()\n",
    "\n",
    "        if device is None:\n",
    "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.device = torch.device(device)\n",
    "        self.model.to(device)\n",
    "\n",
    "    def _mean_pooling(self, model_output, attention_mask):\n",
    "        token_embeddings = model_output.last_hidden_state \n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "    def encode(self, sentences, *, batch_size=8, max_length = 256):\n",
    "        iterator = range(0, len(sentences), batch_size)\n",
    "        all_embeddings = []\n",
    "\n",
    "        for batch_idx in iterator:\n",
    "            batch = sentences[batch_idx:batch_idx + batch_size]\n",
    "            encoded_input = self.tokenizer.batch_encode_plus(\n",
    "                        batch, \n",
    "                        max_length=max_length, \n",
    "                        padding='max_length', \n",
    "                        truncation=True, \n",
    "                        return_tensors='pt'\n",
    "                ).to(self.device)\n",
    "            \n",
    "            # 文章ベクトルを計算\n",
    "            # BERTの最終層の出力を平均を計算する。（ただし、[PAD]は除く。）\n",
    "            with torch.no_grad():\n",
    "                model_output = self.model(**encoded_input)\n",
    "            sentence_embeddings = self._mean_pooling(model_output, encoded_input[\"attention_mask\"]).to('cpu')\n",
    "                \n",
    "            all_embeddings.extend(sentence_embeddings)\n",
    "        return all_embeddings\n",
    "\n",
    "def get_bert_embeddings(texts, *, model=None):\n",
    "    MODEL_NAME = 'cl-tohoku/bert-base-japanese-whole-word-masking'\n",
    "    model = BertJapanese(MODEL_NAME, model=model)\n",
    "    sentence_embeddings = model.encode(texts, batch_size=8)\n",
    "    return sentence_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert([\"こんにちは\"], model=model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'cl-tohoku/bert-base-japanese-whole-word-masking'\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = BertModel.from_pretrained(MODEL_NAME)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(device)\n",
    "model.to(device)\n",
    "\n",
    "# 各データの形式を整える\n",
    "max_length = 256\n",
    "sentence_vectors = [] # 文章ベクトルを追加していく。\n",
    "text = \"こんにちは\"\n",
    "encoding = tokenizer(\n",
    "            text, \n",
    "            max_length=max_length, \n",
    "            padding='max_length', \n",
    "            truncation=True, \n",
    "            return_tensors='pt'\n",
    "        ).to(device)\n",
    "attention_mask = encoding['attention_mask']\n",
    "\n",
    "# 文章ベクトルを計算\n",
    "# BERTの最終層の出力を平均を計算する。（ただし、[PAD]は除く。）\n",
    "with torch.no_grad():\n",
    "  output = model(**encoding)\n",
    "  last_hidden_state = output.last_hidden_state \n",
    "  averaged_hidden_state = \\\n",
    "                (last_hidden_state*attention_mask.unsqueeze(-1)).sum(1) \\\n",
    "                / attention_mask.sum(1, keepdim=True) \n",
    "\n",
    "  # 文章ベクトルとラベルを追加\n",
    "  sentence_vectors.append(averaged_hidden_state[0].cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.47266144e-01, -3.18962038e-01,  4.93316323e-01, -8.47538710e-02,\n",
       "       -3.44220579e-01,  1.09281659e-01, -4.32385057e-01, -2.94819295e-01,\n",
       "        1.88462511e-01, -3.31487030e-01, -5.47910571e-01,  3.16460311e-01,\n",
       "        4.04441923e-01,  2.10141256e-01,  3.19194525e-01, -5.75058162e-01,\n",
       "       -1.61620110e-01,  2.66705424e-01,  8.48741308e-02,  2.62632459e-01,\n",
       "       -2.76574522e-01,  3.18346173e-01, -6.93993926e-01,  1.13959268e-01,\n",
       "       -3.96769345e-01, -1.67217299e-01, -2.08724305e-01, -2.12834805e-01,\n",
       "        6.02680854e-02, -5.74268121e-03,  4.89590257e-01, -5.08453548e-01,\n",
       "       -7.83277333e-01, -6.20669782e-01,  1.61364302e-01, -2.16724724e-01,\n",
       "        9.62714572e-03,  7.87797451e-01,  1.21307835e-01, -1.65233776e-01,\n",
       "        8.03287029e-01,  3.49706352e-01, -8.17347884e-01,  5.30690849e-01,\n",
       "        1.70209855e-01,  1.91455241e-02,  3.20568979e-01,  8.96044895e-02,\n",
       "        3.73670101e-01,  3.11021626e-01,  8.89836192e-01, -1.77655276e-02,\n",
       "        2.06168175e-01, -2.22264096e-01,  5.14466941e-01,  6.57853246e-01,\n",
       "       -2.20017239e-01, -1.19692218e-02,  5.44618309e-01, -2.42894456e-01,\n",
       "        8.69229138e-01, -4.84213740e-01, -7.20199198e-02, -4.90074083e-02,\n",
       "       -2.91473120e-02,  5.81330955e-02,  6.47124499e-02, -6.92438722e-01,\n",
       "       -1.48735598e-01,  4.05511335e-02, -9.09262478e-01, -1.38349682e-01,\n",
       "        3.23428035e-01,  2.30123520e-01,  4.23302591e-01, -5.16927779e-01,\n",
       "        3.90997946e-01, -7.60362029e-01,  3.04852016e-02,  2.27027327e-01,\n",
       "       -2.43498594e-01, -9.15528461e-02, -1.24958279e-02, -3.89495075e-01,\n",
       "        7.78940380e-01, -1.55741781e-01, -8.03593360e-03,  3.94460678e-01,\n",
       "       -1.92316025e-01,  7.75267124e-01,  1.25448573e-02,  1.90445445e-02,\n",
       "       -3.88762623e-01,  9.43345070e-01, -2.40774509e-02, -1.29143357e-01,\n",
       "       -7.37550855e-01,  7.83119023e-01,  1.07622243e-01, -2.74297595e-01,\n",
       "        9.13107693e-01,  3.90864074e-01, -2.05275327e-01, -5.83491564e-01,\n",
       "       -5.59748895e-02,  3.14288318e-01,  5.23050010e-01,  6.73817515e-01,\n",
       "       -5.12913987e-02,  1.10593095e-01,  1.05916902e-01, -4.89726141e-02,\n",
       "       -4.60024104e-02,  4.57382500e-02,  3.91052783e-01,  4.73452091e-01,\n",
       "        1.11343956e+00,  4.88446951e-02,  7.56107926e-01,  2.48899460e-01,\n",
       "       -9.29049179e-02, -5.93533516e-01,  8.19904655e-02,  1.54942032e-02,\n",
       "        3.47359836e-01,  5.21462917e-01,  9.76423085e-01, -1.59255102e-01,\n",
       "       -6.77185833e-01, -5.50413013e-01,  2.90191799e-01,  1.83096617e-01,\n",
       "        6.21053696e-01,  3.69528234e-01,  6.66379869e-01,  1.54634744e-01,\n",
       "        2.69995987e-01, -1.25852302e-01, -2.24516198e-01,  4.84691858e-01,\n",
       "        1.89755663e-01,  2.32715994e-01,  1.93361357e-01, -4.52992842e-02,\n",
       "       -1.57707974e-01,  1.52309891e-02, -4.98974323e-01, -2.08580881e-01,\n",
       "       -2.67328471e-01,  5.17138302e-01, -2.71332681e-01, -5.68661809e-01,\n",
       "        9.75362509e-02,  1.77421659e-01,  3.33241552e-01, -3.38442594e-01,\n",
       "       -4.49221224e-01,  6.49522662e-01,  5.42780638e-01, -2.24360704e-01,\n",
       "        6.71458840e-01,  5.19467413e-01,  6.06835783e-02,  1.12374328e-01,\n",
       "        8.22278857e-03,  4.27073389e-01,  1.09522864e-01,  3.10705844e-02,\n",
       "       -3.44509840e-01,  9.59889591e-01, -7.07138032e-02, -8.62223804e-02,\n",
       "       -1.80749178e-01, -1.64384142e-01,  3.89517784e-01,  1.04744360e-01,\n",
       "        6.78173065e-01, -8.06730986e-02,  6.26160622e-01,  9.48539972e-02,\n",
       "       -5.46129167e-01,  7.13811457e-01,  2.10979372e-01, -2.34017536e-01,\n",
       "       -8.92662644e-01, -2.03427002e-01, -4.29790169e-01,  1.72448277e-01,\n",
       "       -4.76377308e-01, -1.32481098e-01, -5.61137855e-01,  7.37824142e-01,\n",
       "        1.54375032e-01, -1.95158675e-01,  3.44779521e-01, -2.42426276e-01,\n",
       "       -1.65767998e-01,  4.80542570e-01, -1.42869756e-01, -5.16800061e-02,\n",
       "        1.54476851e-01, -3.81093293e-01,  1.02287841e+00, -1.05635524e-01,\n",
       "        3.54934871e-01,  1.51518375e-01,  4.66698736e-01, -6.61553442e-01,\n",
       "       -1.56462580e-01, -4.28341050e-03,  7.91839480e-01,  5.57001114e-01,\n",
       "       -1.98109016e-01,  1.37745607e+00,  6.75734639e-01, -9.18003097e-02,\n",
       "        1.71677738e-01, -4.16698446e-03,  2.70337582e-01, -1.04119682e+00,\n",
       "       -4.29321110e-01, -5.05392432e-01,  2.56656349e-01, -9.51187387e-02,\n",
       "        7.88880408e-01,  1.86654441e-02,  5.03133945e-02,  3.30358207e-01,\n",
       "       -1.00046851e-01,  1.33977145e-01, -8.97482276e-01,  1.42203212e-01,\n",
       "       -6.10485256e-01, -1.07832217e+00,  1.47557944e-01,  2.87084989e-02,\n",
       "        1.82803109e-01, -4.74628538e-01,  8.00373077e-01, -1.03167593e+00,\n",
       "       -3.99596483e-01,  4.08750594e-01, -1.64457962e-01,  1.59125298e-01,\n",
       "        2.66145527e-01,  4.64443117e-01, -1.68761298e-01,  3.02524179e-01,\n",
       "       -3.53842437e-01, -5.80489278e-01,  4.41781804e-02, -1.63136274e-01,\n",
       "        2.36109942e-01, -2.69406706e-01, -6.14991486e-01,  1.95989281e-01,\n",
       "        1.10780433e-01,  7.58104920e-01,  3.10200125e-01,  2.37371568e-02,\n",
       "       -4.67148066e-01,  2.75072038e-01,  2.08522886e-01,  4.12910849e-01,\n",
       "       -1.11575258e+00, -9.03073251e-02, -5.95233798e-01,  1.37282386e-01,\n",
       "       -5.51474690e-01,  1.69940799e-01,  3.29852879e-01, -3.53521496e-01,\n",
       "       -1.04674950e-01, -1.21195160e-01,  4.79036085e-02, -1.00487933e-01,\n",
       "        7.69680962e-02, -2.43631914e-01, -1.57275409e-01,  5.05061209e-01,\n",
       "       -4.59101290e-01,  2.53788471e-01,  5.90306222e-01, -9.31061953e-02,\n",
       "       -1.52483866e-01,  3.47090483e-01, -2.30402380e-01,  5.72092295e-01,\n",
       "        1.66032076e-01, -2.05871984e-01, -2.40735099e-01, -4.11242187e-01,\n",
       "       -2.32333735e-01, -6.24776244e-01,  3.85632128e-01, -5.27529716e-01,\n",
       "       -4.02288496e-01, -8.46633315e-03,  6.26401067e-01, -4.22387362e-01,\n",
       "       -7.58442134e-02, -3.13547552e-01, -4.70502675e-01,  5.36072969e-01,\n",
       "       -1.18367098e-01, -4.15847212e-01,  1.04004763e-01, -8.85880828e-01,\n",
       "        1.16828857e-02, -5.31222522e-01, -3.69636714e-01,  2.46373653e-01,\n",
       "        5.67714632e-01,  6.22569680e-01,  7.45094940e-02,  5.64614423e-02,\n",
       "        3.22443783e-01,  2.53971219e-01, -6.24087036e-01,  3.65918763e-02,\n",
       "        5.90883553e-01, -2.46413618e-01,  1.31205410e-01, -2.69760698e-01,\n",
       "        1.12605438e-01,  4.32672739e-01, -8.47162753e-02, -2.57667959e-01,\n",
       "       -7.77504802e-01, -1.47199661e-01,  7.11841136e-02, -1.17488648e-03,\n",
       "        4.93438721e-01,  1.27893925e-01,  2.46748880e-01, -1.41799375e-01,\n",
       "       -1.25022128e-01,  7.81533360e-01, -2.19390243e-01,  7.52695680e-01,\n",
       "       -2.43519142e-01,  1.18059255e-01,  2.87434995e-01, -8.29529107e-01,\n",
       "       -3.11036587e-01, -2.27896832e-02,  3.64911318e-01, -2.47766882e-01,\n",
       "       -2.26479657e-02,  1.70211360e-01, -4.84278984e-02,  3.42011213e-01,\n",
       "        9.58981812e-02, -1.92423552e-01, -4.65180218e-01, -9.12701041e-02,\n",
       "       -9.96747836e-02,  1.35086209e-01,  3.33576724e-02,  2.35691637e-01,\n",
       "        5.10284483e-01, -4.12555605e-01,  3.72241199e-01, -8.96547318e-01,\n",
       "        8.71284977e-02,  8.11963558e-01, -1.98377118e-01, -3.79273817e-02,\n",
       "        3.51873562e-02, -6.00242615e-01,  6.95397854e-02, -6.19570613e-01,\n",
       "        1.88917547e-01, -4.03032303e-01,  4.55216169e-01,  5.27912080e-01,\n",
       "        2.86499947e-01,  1.85142651e-01,  2.41059251e-02, -7.51920879e-01,\n",
       "        2.59257019e-01, -1.65905412e-02, -4.35133368e-01,  1.96381897e-01,\n",
       "        6.53827488e-01, -6.84835076e-01, -9.06357408e-01, -5.19145310e-01,\n",
       "        8.93098339e-02, -2.61391997e-01,  3.33049327e-01, -3.11294794e-01,\n",
       "        5.60306907e-01, -4.17000540e-02, -6.96205914e-01,  2.25631237e-01,\n",
       "        1.14892244e-01, -4.59761322e-01,  6.65604889e-01, -4.28438097e-01,\n",
       "        1.63143799e-01,  9.13374305e-01, -2.48762965e-02, -1.02664992e-01,\n",
       "       -1.68389365e-01,  5.97263277e-01, -9.68967844e-03,  2.56297998e-02,\n",
       "       -5.49216747e-01, -2.81554401e-01, -1.08384058e-01,  3.21632475e-01,\n",
       "        1.18605420e-01, -1.13333620e-01, -1.12389982e+00, -2.93429345e-01,\n",
       "       -3.84683073e-01,  4.10915136e-01,  1.76177442e-01,  7.25268945e-02,\n",
       "        2.88713783e-01,  3.11594665e-01,  1.00781679e-01,  4.11024719e-01,\n",
       "       -2.00253576e-01, -8.97109360e-02, -9.00614262e-02, -6.76982224e-01,\n",
       "       -2.89274007e-02, -1.02998185e+00, -6.11011386e-01,  1.07071447e+00,\n",
       "        5.50244689e-01,  4.49288845e-01, -2.85164773e-01,  6.99505687e-01,\n",
       "        4.47158813e-01, -4.11648452e-01, -1.60394758e-01,  1.94195583e-01,\n",
       "        7.71591902e-01,  2.60661215e-01, -4.04605001e-01,  3.98814142e-01,\n",
       "        8.45554709e-01, -8.21944535e-01, -1.44105047e-01,  2.37833887e-01,\n",
       "       -2.73901880e-01,  1.40800640e-01,  2.29852036e-01,  7.23626494e-01,\n",
       "        4.40160900e-01, -2.38075014e-02,  4.28274632e-01, -4.68777269e-01,\n",
       "        1.59541279e-01, -8.15583408e-01,  1.47773936e-01, -5.44634759e-01,\n",
       "       -6.61876798e-01,  1.98723763e-01, -2.29177669e-01,  1.20184049e-01,\n",
       "       -5.79003036e-01,  3.52086201e-02, -1.04087424e+00,  3.61116230e-01,\n",
       "       -3.52368876e-02, -2.94111483e-02, -5.59295639e-02, -1.50576830e-01,\n",
       "        4.71323341e-01,  5.73567092e-01, -6.53672040e-01,  4.35100168e-01,\n",
       "        5.29974625e-02,  5.09704947e-01, -4.06625792e-02, -3.61443818e-01,\n",
       "       -1.58047050e-01, -5.89261413e-01,  2.10727572e-01, -3.71280611e-01,\n",
       "       -1.44498304e-01,  1.62194639e-01,  2.51135416e-02, -9.62445080e-01,\n",
       "        6.62756860e-02, -9.87354964e-02,  2.35651702e-01, -4.28316519e-02,\n",
       "       -7.52810597e-01, -5.82573652e-01,  3.42642486e-01, -7.97130287e-01,\n",
       "       -2.54221559e-02,  8.17355931e-01,  3.73032153e-01, -5.41748226e-01,\n",
       "       -3.85670289e-02, -1.94231197e-01,  1.08722687e-01, -4.96822536e-01,\n",
       "        1.43056750e-01, -2.87111044e-01, -4.26910579e-01, -4.45035368e-01,\n",
       "        2.60990500e-01,  7.33527690e-02,  5.54589987e-01, -4.92341101e-01,\n",
       "       -7.18865022e-02, -9.29414406e-02,  3.45519111e-02, -1.61731631e-01,\n",
       "        6.23383343e-01,  1.35407060e-01,  4.13564533e-01, -5.61543107e-02,\n",
       "        4.06597197e-01,  1.56097904e-01,  9.28880572e-01,  5.94804764e-01,\n",
       "       -2.10576445e-01,  1.09778926e-01, -2.79862076e-01, -3.47318411e-01,\n",
       "       -2.10503966e-01, -1.22288808e-01, -1.71635821e-01,  8.19490571e-03,\n",
       "       -1.31450370e-02, -4.91203696e-01, -2.90254772e-01, -2.34734826e-02,\n",
       "       -2.13442609e-01, -1.05809197e-01, -3.31920013e-02, -2.16868669e-01,\n",
       "        1.26622379e-01,  4.62208271e-01, -3.03816974e-01, -5.19942403e-01,\n",
       "        5.94963431e-01, -5.92030063e-02, -2.65047967e-01, -8.13689828e-02,\n",
       "        1.12299323e-02, -2.82675922e-01,  6.32387757e-01,  1.55589193e-01,\n",
       "       -7.17962757e-02,  2.61058480e-01,  2.78896570e-01, -5.05096316e-01,\n",
       "       -1.15216352e-01, -3.91745269e-01,  3.19098011e-02,  5.26770234e-01,\n",
       "       -4.35459390e-02, -1.16137251e-01,  2.93428808e-01,  1.54145656e-03,\n",
       "       -4.72765505e-01,  1.36828154e-01,  9.03006047e-02,  1.66562349e-01,\n",
       "       -3.81761312e-01, -8.12134519e-02, -6.84893310e-01, -3.42641324e-01,\n",
       "        3.61885041e-01, -6.34383142e-01, -3.13572764e-01,  3.99512351e-01,\n",
       "       -6.96054995e-02, -1.43118471e-01,  4.33675289e-01,  6.80875421e-01,\n",
       "        2.55068034e-01, -5.35894096e-01, -3.84703577e-01, -4.27050889e-01,\n",
       "       -5.56655228e-02, -5.47507167e-01,  1.94345906e-01, -5.91338992e-01,\n",
       "        1.75766140e-01,  1.37191355e-01,  9.00745392e-02,  4.44989979e-01,\n",
       "        2.90825218e-01, -2.40759805e-01,  4.07615364e-01,  8.74071300e-01,\n",
       "       -2.23951131e-01, -4.40313905e-01,  1.30257145e-01,  8.58387351e-01,\n",
       "        4.12435353e-01,  3.65981191e-01, -4.13511455e-01, -7.00444341e-01,\n",
       "        7.54688084e-01,  4.17063892e-01, -1.89476267e-01, -2.60136485e-01,\n",
       "        1.17141724e-01,  1.30248964e-01,  3.43322046e-02,  3.43685508e-01,\n",
       "        5.84462583e-01, -5.93093574e-01,  7.52735555e-01, -1.10138059e+00,\n",
       "        2.30740160e-01,  7.43030262e+00, -4.36883122e-01,  6.41959682e-02,\n",
       "       -2.05043763e-01,  2.53398955e-01,  7.78130442e-02, -2.64994121e+00,\n",
       "        6.33002400e-01, -6.11111164e-01, -5.90241663e-02, -2.13333607e-01,\n",
       "       -1.14125744e-01, -1.26709849e-01,  7.77721629e-02, -2.16378018e-01,\n",
       "        9.16933492e-02,  2.72726808e-02, -3.64251554e-01, -6.23825908e-01,\n",
       "        4.88657534e-01, -7.61502624e-01,  3.37317407e-01, -6.07685268e-01,\n",
       "       -5.96791565e-01,  1.83967024e-01, -2.04670504e-01,  2.44728521e-01,\n",
       "        5.90628982e-01, -6.38189763e-02, -7.87975669e-01,  7.11053014e-01,\n",
       "       -2.17454150e-01, -6.11372590e-01, -1.13471508e-01,  1.02106892e-01,\n",
       "        3.73823971e-01, -2.97982246e-01, -2.38623649e-01, -1.72373027e-01,\n",
       "        1.92405969e-01, -7.43635297e-01, -9.29768085e-02, -2.98622958e-02,\n",
       "        2.79660940e-01,  6.93994984e-02, -5.72276711e-01, -5.70602894e-01,\n",
       "       -2.62666166e-01,  4.68082018e-02,  3.41801345e-01,  5.28922565e-02,\n",
       "       -9.25206780e-01,  4.33312505e-01,  4.09857899e-01,  8.70287240e-01,\n",
       "       -6.04406014e-05, -2.69555543e-02, -2.65719086e-01,  7.12159455e-01,\n",
       "       -4.95501906e-01,  3.01941276e-01, -4.29048389e-01, -4.80387121e-01,\n",
       "       -7.81360269e-01, -1.36104256e-01, -2.82770663e-01,  5.63628078e-01,\n",
       "        3.32721770e-01,  2.62043178e-01,  3.09413135e-01,  1.59769893e-01,\n",
       "       -8.54703605e-01,  3.12568188e-01, -7.37917781e-01, -4.88509834e-01,\n",
       "       -1.01980709e-01, -8.01110744e-01,  3.27919215e-01, -5.85542500e-01,\n",
       "       -3.52598727e-01,  3.54507118e-01,  6.79206550e-01,  2.97255933e-01,\n",
       "       -3.64825308e-01, -7.43390083e-01,  4.63230014e-02,  1.27023105e-02,\n",
       "        3.87907863e-01,  1.03039289e+00,  5.94004989e-01,  3.84054989e-01,\n",
       "        1.87349230e-01,  7.82738626e-01,  3.08079123e-01,  9.45566669e-02,\n",
       "        3.01350057e-01,  1.17464289e-01, -3.94835502e-01, -8.45492855e-02,\n",
       "        2.37084061e-01,  3.32834661e-01, -4.89512771e-01,  4.70377132e-02,\n",
       "       -8.66248235e-02,  4.60289240e-01, -1.58422619e-01,  1.78342432e-01,\n",
       "       -4.12354141e-01, -5.70313811e-01,  8.26677755e-02, -9.35820788e-02,\n",
       "       -4.48484957e-01, -2.43224666e-01,  1.96501669e-02, -2.42961079e-01,\n",
       "        9.13050771e-01,  1.05667852e-01,  2.22959906e-01, -1.89356029e-01,\n",
       "        6.21579111e-01, -2.23688513e-01,  1.72614694e-01,  6.89376175e-01,\n",
       "       -5.84115796e-02, -2.91824818e-01, -9.07482356e-02,  1.82986811e-01,\n",
       "        2.69772997e-03,  2.43656963e-01, -3.75650972e-01,  4.94912148e-01,\n",
       "        1.08682632e-01,  3.27915251e-01, -2.77663887e-01, -4.01521206e-01,\n",
       "       -5.28006479e-02,  2.25120068e-01, -2.80157745e-01, -9.59475040e-01,\n",
       "       -9.17148232e-01,  4.97118562e-01,  2.93744266e-01, -3.17606062e-01,\n",
       "        7.99783021e-02, -2.20162012e-02, -5.80368638e-01, -1.54358768e+00,\n",
       "        5.91704428e-01,  1.00845434e-01,  5.63025996e-02,  2.51483381e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_vectors[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering用の関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-meansでクラスタ分析。とりあえず9つのグループに分けてみる\n",
    "def kmeans_clustering(vectors, *, n_clusters=9):\n",
    "  km_model = KMeans(n_clusters=n_clusters, random_state = 0)\n",
    "  km_model.fit(vectors)\n",
    "  return km_model.labels_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考：教師あり（トリグラム、ナイーブベイズ）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参考 \n",
    "def supervised_naivebayse_vector(texts, *, model=None, train_text = train_df[\"title\"], train_category=train_df[\"category\"]):\n",
    "  if model is None:\n",
    "    model = make_pipeline(\n",
    "      TfidfVectorizer(\n",
    "                      analyzer=\"char\"\n",
    "                      , ngram_range=(3,3)\n",
    "                      , max_df=0.9\n",
    "                      , min_df = 5)\n",
    "      , MultinomialNB()\n",
    "    )\n",
    "    model.fit(train_text, train_category)\n",
    "  return model.predict(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ngram tfidf, kmeans\n",
    "X = ngram_tfidf(test_df[\"title\"])\n",
    "test_labels = kmeans_clustering(X)\n",
    "test_df[\"pred_trigram_tfidf_kmeans\"] = test_labels\n",
    "\n",
    "# word tfidf, kmeans\n",
    "X = word_tfidf(test_df[\"title\"])\n",
    "test_labels = kmeans_clustering(X)\n",
    "test_df[\"pred_word_tfidf_kmeans\"] = test_labels\n",
    "\n",
    "# fasttext, kmeans\n",
    "# モデルは以下からダウンロード\n",
    "# https://fasttext.cc/docs/en/crawl-vectors.html\n",
    "#ft = fasttext.load_model('fasttext/cc.ja.300.bin')\n",
    "vectors = fasttext_vector(test_df[\"title\"], model = ft)\n",
    "test_labels = kmeans_clustering(vectors)\n",
    "test_df[\"pred_fasttext_kmeans\"] = test_labels\n",
    "\n",
    "# sentence bert, kmeans\n",
    "embedding_binary_path = \"embedding/sentencebert_embedding.pickle\"\n",
    "if Path(embedding_binary_path).exists():\n",
    "  with open(embedding_binary_path, \"rb\") as f:\n",
    "    sentence_embeddings = pickle.load(f)\n",
    "else:\n",
    "  sentence_embeddings = sentencebert(test_df[\"title\"])\n",
    "  with open(\"embedding/sentencebert_embedding.pickle\", \"wb\") as f:\n",
    "    pickle.dump(sentence_embeddings.detach().numpy(), f)\n",
    "\n",
    "test_labels = kmeans_clustering(sentence_embeddings)\n",
    "test_df[\"pred_sentencebert_kmeans\"] = test_labels\n",
    "\n",
    "test_labels = supervised_naivebayse_vector(test_df[\"title\"])\n",
    "test_df[\"pred_supervised_naivebayse\"] = test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "OUT_DIR = \"prediction\"\n",
    "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "test_df.to_csv(str(Path(OUT_DIR).joinpath(\"livedoor.csv\")), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngram tfidf, kmeans\n",
      "{'ct_cf_tt_tf': (30397, 240819, 172013, 99203), 'tp_fp_tn_fn': (21785, 150228, 90591, 8612), 'precision': 0.12664740455663234, 'recall': 0.7166825673586209, 'f1': 0.21525616323304184, 'accuracy': 0.4143413367942894}\n",
      "word tfidf, kmeans\n",
      "{'ct_cf_tt_tf': (30397, 240819, 85461, 185755), 'tp_fp_tn_fn': (12802, 72659, 168160, 17595), 'precision': 0.14979932366810592, 'recall': 0.4211599828930487, 'f1': 0.22099466588409952, 'accuracy': 0.667224647513421}\n",
      "fasttext, kmeans\n",
      "{'ct_cf_tt_tf': (30397, 240819, 33096, 238120), 'tp_fp_tn_fn': (6382, 26714, 214105, 24015), 'precision': 0.19283297075175249, 'recall': 0.20995492976280555, 'f1': 0.2010300348069866, 'accuracy': 0.812957200165182}\n",
      "sentencebert, kmeans\n",
      "{'ct_cf_tt_tf': (30397, 240819, 32532, 238684), 'tp_fp_tn_fn': (8420, 24112, 216707, 21977), 'precision': 0.25882208287224884, 'recall': 0.277001019837484, 'f1': 0.2676031718285687, 'accuracy': 0.8300653353784437}\n",
      "supervised, naivebayse\n",
      "{'ct_cf_tt_tf': (30397, 240819, 31432, 239784), 'tp_fp_tn_fn': (19530, 11902, 228917, 10867), 'precision': 0.6213413082209214, 'recall': 0.6424976148962068, 'f1': 0.6317423862588752, 'accuracy': 0.9160484632175093}\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv(\"prediction/livedoor.csv\")\n",
    "cluster_similarity_func = cluster_similarity_pair\n",
    "print(\"ngram tfidf, kmeans\")\n",
    "print(cluster_similarity_func(test_df[\"category\"], test_df[\"pred_trigram_tfidf_kmeans\"]))\n",
    "print(\"word tfidf, kmeans\")\n",
    "print(cluster_similarity_func(test_df[\"category\"], test_df[\"pred_word_tfidf_kmeans\"]))\n",
    "print(\"fasttext, kmeans\")\n",
    "print(cluster_similarity_func(test_df[\"category\"], test_df[\"pred_fasttext_kmeans\"]))\n",
    "print(\"sentencebert, kmeans\")\n",
    "print(cluster_similarity_func(test_df[\"category\"], test_df[\"pred_sentencebert_kmeans\"]))\n",
    "print(\"supervised, naivebayse\")\n",
    "print(cluster_similarity_func(test_df[\"category\"], test_df[\"pred_supervised_naivebayse\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purity-Inverse Purity F-measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngram tfidf, kmeans\n",
      "{'precision': 0.2903663500678426, 'recall': 0.8208955223880597, 'f1': 0.4289905782443096}\n",
      "word tfidf, kmeans\n",
      "{'precision': 0.3514246947082768, 'recall': 0.582089552238806, 'f1': 0.43825928497049643}\n",
      "fasttext, kmeans\n",
      "{'precision': 0.3147896879240163, 'recall': 0.34328358208955223, 'f1': 0.32841975688567476}\n",
      "sentencebert, kmeans\n",
      "{'precision': 0.41112618724559025, 'recall': 0.4056987788331072, 'f1': 0.4083944517821643}\n",
      "supervised, naivebayse\n",
      "{'precision': 0.7815468113975577, 'recall': 0.7815468113975577, 'f1': 0.7815468113975577}\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv(\"prediction/livedoor.csv\")\n",
    "cluster_similarity_func = cluster_similarity_purity\n",
    "print(\"ngram tfidf, kmeans\")\n",
    "print(cluster_similarity_func(test_df[\"category\"], test_df[\"pred_trigram_tfidf_kmeans\"]))\n",
    "print(\"word tfidf, kmeans\")\n",
    "print(cluster_similarity_func(test_df[\"category\"], test_df[\"pred_word_tfidf_kmeans\"]))\n",
    "print(\"fasttext, kmeans\")\n",
    "print(cluster_similarity_func(test_df[\"category\"], test_df[\"pred_fasttext_kmeans\"]))\n",
    "print(\"sentencebert, kmeans\")\n",
    "print(cluster_similarity_func(test_df[\"category\"], test_df[\"pred_sentencebert_kmeans\"]))\n",
    "print(\"supervised, naivebayse\")\n",
    "print(cluster_similarity_func(test_df[\"category\"], test_df[\"pred_supervised_naivebayse\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "peachy            26\n",
       "dokujo-tsushin     8\n",
       "livedoor-homme     5\n",
       "kaden-channel      3\n",
       "smax               1\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.query(\"pred_sentencebert_kmeans == 5\")[\"category\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BCubed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngram tfidf, kmeans\n",
      "{'precision': 0.7302436789202665, 'recall': 0.2562694948629095, 'f1': 0.3793951945032219}\n",
      "word tfidf, kmeans\n",
      "{'precision': 0.4351869409102146, 'recall': 0.27031032765763474, 'f1': 0.3334825798222082}\n",
      "fasttext, kmeans\n",
      "{'precision': 0.21664901046033372, 'recall': 0.1995891309097131, 'f1': 0.2077694637396775}\n",
      "sentencebert, kmeans\n",
      "{'precision': 0.2831008772589822, 'recall': 0.27139126072269565, 'f1': 0.2771224287171229}\n",
      "supervised, naivebayse\n",
      "{'precision': 0.6364946721625434, 'recall': 0.629575059396474, 'f1': 0.6330159564573605}\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv(\"prediction/livedoor.csv\")\n",
    "cluster_similarity_func = cluster_similarity_bcubed\n",
    "print(\"ngram tfidf, kmeans\")\n",
    "print(cluster_similarity_func(test_df[\"category\"], test_df[\"pred_trigram_tfidf_kmeans\"]))\n",
    "print(\"word tfidf, kmeans\")\n",
    "print(cluster_similarity_func(test_df[\"category\"], test_df[\"pred_word_tfidf_kmeans\"]))\n",
    "print(\"fasttext, kmeans\")\n",
    "print(cluster_similarity_func(test_df[\"category\"], test_df[\"pred_fasttext_kmeans\"]))\n",
    "print(\"sentencebert, kmeans\")\n",
    "print(cluster_similarity_func(test_df[\"category\"], test_df[\"pred_sentencebert_kmeans\"]))\n",
    "print(\"supervised, naivebayse\")\n",
    "print(cluster_similarity_func(test_df[\"category\"], test_df[\"pred_supervised_naivebayse\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考文献／記事"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "文章のベクトル化\n",
    "* [SudachiPy](https://github.com/WorksApplications/SudachiPy/blob/develop/docs/tutorial.md)\n",
    "* [tf-idfでベクトル化したラジオ感想ツイートをクラスタリングして可視化する](https://note.com/himaratsu/n/necefee6e5454)\n",
    "* [機械学習 〜 テキスト分類（ナイーブベイズ分類器） 〜](https://qiita.com/fujin/items/39d450b910bf2be866b5)\n",
    "* [fastTextとDoc2Vecのモデルを作成してニュース記事の多クラス分類の精度を比較する](https://qiita.com/kazuki_hayakawa/items/ca5d4735b9514895e197)\n",
    "* [【日本語モデル付き】2020年に自然言語処理をする人にお勧めしたい文ベクトルモデル](https://qiita.com/sonoisa/items/1df94d0a98cd4f209051)\n",
    "* [https://huggingface.co/sonoisa/sentence-bert-base-ja-mean-tokens-v2](https://huggingface.co/sonoisa/sentence-bert-base-ja-mean-tokens-v2)\n",
    "\n",
    "エラー・デバッグ関係\n",
    "* [Pytorch: Can't call numpy() on Variable that requires grad. Use var.detach().numpy() instead](https://stackoverflow.com/questions/55466298/pytorch-cant-call-numpy-on-variable-that-requires-grad-use-var-detach-num)\n",
    "  * sentence-bertの出力をkmeansに入力したらエラーが出たときの解消方法\n",
    "\n",
    "\n",
    "クラスタリング結果の比較方法\n",
    "* [２つのクラスタリング結果がどのくらい似ているかの指標](https://takemikami.com/2019/02/25/clustdiff.html)\n",
    "* [Precision and recall for clustering?](https://stats.stackexchange.com/questions/15158/precision-and-recall-for-clustering/80194)\n",
    "* [Evaluation of clustering](https://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-clustering-1.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ee66785c2d12082be357f60b53b6454e1233dc7704302fcc0105c907ee27f10e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
